{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z1l_y34lYp_5",
    "outputId": "aba5fb57-3016-44f2-f4e0-1594844c0518"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade langchain langchain_google_genai langchain-core langchain_community docs2txt pypdf langchain_chroma sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo_4IZipMY_M"
   },
   "source": [
    "##**What is Retrieval Augmented Generation (RAG)?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP_y8KpHOGHz"
   },
   "source": [
    "###**LangChain Components and Expression Language (LCEL)**\n",
    "1. Large Language Model (LLM)\n",
    "2. Output Parsers\n",
    "3. Structured Output\n",
    "4. Prompt Templates\n",
    "5. LLM Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8RLGDPJYGsB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCB54yqcYRkX"
   },
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"\"\n",
    "os.environ['LANGCHAIN_PROJECT']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHEqUfrLYuEu",
    "outputId": "9bc424b4-10c5-4e9f-a416-d8fb0d65a4fc"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llm_response=llm.invoke(\"Tell me a simple joke about coding!\")\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "nmzII1DvZXFT",
    "outputId": "0b079d60-a38c-4dbe-d630-019f1eb08f30"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "output_parser.invoke(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "QyS-55JkZpID",
    "outputId": "57b6de46-8e54-4cd4-c6a3-b10ecb906d94"
   },
   "outputs": [],
   "source": [
    "chain=llm | output_parser\n",
    "chain.invoke(\"Tell me a story!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7zo3pencQ0k",
    "outputId": "90610363-0cef-4e9d-fa78-052aa9522c71"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class ReviewDSA(BaseModel):\n",
    "  topic:str=Field(description=\"Name of the DSA topic\")\n",
    "  summary: str=Field(description=\"Brief summary of the review\")\n",
    "  rating: float=Field(description=\"Overall rating out of 5\")\n",
    "  pros: List[str]=Field(description=\"List of positive aspects\")\n",
    "\n",
    "\n",
    "prompt_text='''\n",
    " Just started learning Dynamic Programming, and wow, this topic is mind-blowing! The way it optimizes\n",
    "    recursive solutions is just amazing. It really helps in solving problems efficiently that involve\n",
    "    overlapping subproblems, like Fibonacci and Knapsack.\n",
    "\n",
    "    But not gonna lie, it's tough at first. You really need to practice a lot to get the hang of thinking\n",
    "    in terms of subproblems. Also, memorization techniques can be confusing, and it’s easy to mess up\n",
    "    the transition from recursion to tabulation.\n",
    "\n",
    "    Overall, I'd rate it a 4.5 out of 5. Once you get it, it’s a game-changer for problem-solving.\n",
    "    Definitely worth the effort!\n",
    "'''\n",
    "\n",
    "structured_llm=llm.with_structured_output(ReviewDSA)\n",
    "output=structured_llm.invoke(prompt_text)\n",
    "print(output)\n",
    "print(output.pros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtbUns1PdxJZ",
    "outputId": "78d48f3e-aa52-41f4-bfa9-a07537ec8aeb"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt.invoke({\"topic\":\"sports\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OO_qAo5FeSN9",
    "outputId": "bea83705-3e18-4db2-c95b-87901db71876"
   },
   "outputs": [],
   "source": [
    "chain=prompt | llm|output_parser\n",
    "result=chain.invoke({\"topic\":\"coding\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfFKwB6teoW_",
    "outputId": "5a029f79-b230-4970-a76d-9ee75fa4b073"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a helpful coding assistant!\"),\n",
    "    HumanMessage(content=\"Tell me a joke about Debugging in code\")\n",
    "]\n",
    "\n",
    "response=llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrUwomcCfSrV",
    "outputId": "38f771f8-9fca-49bc-ce79-8758e0986205"
   },
   "outputs": [],
   "source": [
    "template=ChatPromptTemplate([\n",
    "    (\"system\",\"You are a helpful coding assistant\"),\n",
    "    (\"human\",\"tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "chain=template | llm\n",
    "result=chain.invoke({\"topic\":\"coding\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5ugOJQdlY5P",
    "outputId": "42a5b82d-d0b5-42d4-cdc7-337c196fc6ef"
   },
   "outputs": [],
   "source": [
    "!pip install Docx2txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhrFua_-OJFH"
   },
   "source": [
    "###**Document Processing**\n",
    "- Loading Documents\n",
    "- Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkrSRyMZlYcx",
    "outputId": "baf0beba-932f-46fe-ba6c-5ed90623f3e3"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200,length_function=len)\n",
    "\n",
    "        \"docx_loader=Docx2txtLoader(\"/content/docs/SAMPLE_DOCUMENT.docx\")\n",
    "\",\n",
    "documents=docx_loader.load()\n",
    "\n",
    "\n",
    "splits=text_splitter.split_documents(documents)\n",
    "print(f\"splitted with {len(splits)} chunks\")\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQtVbCV6mk1L",
    "outputId": "5c451af9-d6b9-46b2-ac3a-089fe444377f"
   },
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bj9Ylj4mrtm",
    "outputId": "97a11c58-f617-4587-8d7a-b45768605783"
   },
   "outputs": [],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oEFMSgYnI11",
    "outputId": "721b96d8-cc55-4564-b564-8802b4002cc4"
   },
   "outputs": [],
   "source": [
    "def load_documents(folder_path : str)-> List[Document]:\n",
    "  documnets=[]\n",
    "  for filename in os.listdir(folder_path):\n",
    "    file_path=os.path.join(folder_path,filename)\n",
    "    if filename.endswith('.pdf'):\n",
    "      loader=PyPDFLoader(file_path)\n",
    "    elif filename.endswith('.docx'):\n",
    "      loader=Docx2txtLoader(file_path)\n",
    "    else:\n",
    "      print(f\"unsupported file type : {filename}\")\n",
    "      continue\n",
    "    documents.extend(loader.load())\n",
    "  return documents\n",
    "\n",
    "folder_path='/content/docs'\n",
    "documents=load_documents(folder_path)\n",
    "print(f\"loaded {len(documents)} from folder\")\n",
    "\n",
    "splits=text_splitter.split_documents(documents)\n",
    "print(f\"splitted documents into {len(splits)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGNuGx6XtI6y",
    "outputId": "39c3cf47-fc76-4792-ad84-c70e7bdcf764"
   },
   "outputs": [],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCilWgP5tMmN",
    "outputId": "bfbd02be-3b72-46e9-cb15-e2f001f27890"
   },
   "outputs": [],
   "source": [
    "splits[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG6ZVjF7OLK1"
   },
   "source": [
    "\n",
    "###**Creating Embeddings**\n",
    "- Using GoogleAI Embeddings\n",
    "- Using SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFFBX_nVtR-O",
    "outputId": "b333b4c3-2d9a-4e01-ccb1-6ccdfa07c236"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "document_embedding=embedding.embed_documents([split.page_content for split in splits])\n",
    "print(f\"created embedding for {len(document_embedding)} documents chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srsOkJA6OM-n"
   },
   "source": [
    "\n",
    "###**Setting Up the Vector Store**\n",
    "- Creating the Vector Store\n",
    "- Performing Similarity Search\n",
    "- Creating a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1e986bVvm58",
    "outputId": "e1dd4070-f617-4aad-af32-084acbc35217"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "embedding_function=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "collection_name=\"my_collection\"\n",
    "vectorstore=Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    documents=splits,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"vector store created and persisted to './chroma_db' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMqBIjYAwU8y",
    "outputId": "da678fe3-fef7-4459-fb9d-bf633d46571f"
   },
   "outputs": [],
   "source": [
    "query=\"what is the resume summary?\"\n",
    "\n",
    "search_results=vectorstore.similarity_search(query,k=2)\n",
    "print(f\"\\n Top 2 most relevant search for the query : '{query}' \\n \")\n",
    "for i,result in enumerate(search_results,1):\n",
    "  print(f\"result {i}:\")\n",
    "  print(f\"source: {result.metadata.get('source','Unknown')}\")\n",
    "  print(f\"content : {result.page_content}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydsl2l7txjsy",
    "outputId": "f95fead7-ac2e-42b8-9f53-ced32510505a"
   },
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\":2})\n",
    "retriever_results=retriever.invoke(\"who is the candidate whose cv we are seeing?\")\n",
    "print(retriever_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOTwj2MEOO7j"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###**Building the RAG Chain**\n",
    "- Creating the RAG Chain\n",
    "- Using the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwUCOrF4yKO3"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template=\"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "Question:{question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def docs2str(docs):\n",
    "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain=(\n",
    "    {\"context\":retriever | docs2str,\"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    |StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LY1Appk3zabH",
    "outputId": "b1e436fd-09ea-471d-fe3a-5af5500d6505"
   },
   "outputs": [],
   "source": [
    "question=\"which college does the person studies?\"\n",
    "response=rag_chain.invoke(question)\n",
    "print(f\"question: {question}\")\n",
    "print(f\"response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIjUc_EqOQsV"
   },
   "source": [
    "\n",
    "###**Handling Follow-Up Questions**\n",
    "- Creating a History-Aware Retriever\n",
    "- Using the History-Aware RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX3EX_tj0GfU",
    "outputId": "ac946c4f-7caa-4ea1-c9db-8557482a4a9b"
   },
   "outputs": [],
   "source": [
    "from decimal import Context\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "context_q_system_prompt=\"\"\"\n",
    "Given a chat history and latest user question\n",
    " which might refernce context in chat history,\n",
    "formulate a standalone quesion which can be understood without the chat history.\n",
    "Do not answer the question,\n",
    "just formulate it if needed and otherwise return as it is.\n",
    "\"\"\"\n",
    "\n",
    "context_q_prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",context_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\",\"{input}\"),\n",
    "])\n",
    "\n",
    "context_chain=context_q_prompt | llm | StrOutputParser()\n",
    "print(context_chain.invoke({\"input\":\"what does his cgpa? \",\"chat_history\":[]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ib_keJmA19p2"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "history_aware_retriever=create_history_aware_retriever(\n",
    "    llm,retriever,context_q_prompt\n",
    ")\n",
    "\n",
    "qa_prompt=ChatPromptTemplate.from_messages([\n",
    "     (\"system\",\"you are a helpfull AI assistant. use following context to answer the user's question.\"),\n",
    "     (\"system\",\"Context:{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\",\"{input}\"),\n",
    "])\n",
    "\n",
    "question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
    "rag_chain=create_retrieval_chain(history_aware_retriever,question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ajt8dhr325CI",
    "outputId": "6113f3bd-6f49-47c4-adfc-88a67a65332c"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "chat_history=[]\n",
    "question1=\"where was his college?\"\n",
    "answer1=rag_chain.invoke({\"input\":question1,\"chat_history\":chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=answer1)\n",
    "])\n",
    "\n",
    "print(question1)\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBUDqmSe3vyS",
    "outputId": "ece25d79-317f-4003-951a-271a271d39e5"
   },
   "outputs": [],
   "source": [
    "question2=\"what was his sgpa and his college name?\"\n",
    "answer2=rag_chain.invoke({\"input\":question2,\"chat_history\":chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question2),\n",
    "    AIMessage(content=answer2)\n",
    "])\n",
    "\n",
    "print(question2)\n",
    "print(answer2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
